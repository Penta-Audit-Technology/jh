{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 데싸노트의 실전에서 통하는 머신러닝 (골든레빗 MUST HAVE 시리즈)\n",
    "- XGBoost\n",
    "- 부스팅 : 순차적으로 트리를 만들어 이전 트리로부터 더 나은 트리를 만드는 알고리즘\n",
    "- XGBoost, LightGBM, CatBoost 등등\n",
    "- XGBoost의 경우 손실함수와 모형 복잡도까지 같이 고려함\n",
    "- 여담으로 케글에서 사랑받는 부스팅 모델\n",
    "\n",
    "\n",
    "##### 그래서 뭐가 좋아요\n",
    "- 예측 속도가 빠르고 예측력도 좋음\n",
    "- 변수 종류가 많고 데이터가 클 수록 상대적으로 뛰어남\n",
    "\n",
    "\n",
    "##### 이게 아쉬워요\n",
    "- 복잡한 모델이라 해석에 어려움 있음\n",
    "- 하이퍼파라미터 튜닝이 까다로움\n",
    "\n",
    "\n",
    "##### 이럴 때 써먹어요\n",
    "- 종속변수가 연속형이든 범주형이든 가리지 않음\n",
    "- 이미지나 자연어가 아니라 표로 정리된 데이터라면 거의 모둔 상황에 사용 가능함\n",
    "\n",
    "\n",
    "##### 오늘은 뭐해요\n",
    "- 개인 취향에 따른 커플 성사가 잘 되는지 예측해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교재에는 없는데 혹시라도 나올 수 있는 경고가 짜증나니 추가함\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 가져옵니다\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "file_url = 'https://media.githubusercontent.com/media/musthave-ML10/data_source/main/dating.csv'\n",
    "data = pd.read_csv(file_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>has_null</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>age_o</th>\n",
       "      <th>race</th>\n",
       "      <th>race_o</th>\n",
       "      <th>importance_same_race</th>\n",
       "      <th>importance_same_religion</th>\n",
       "      <th>pref_o_attractive</th>\n",
       "      <th>pref_o_sincere</th>\n",
       "      <th>...</th>\n",
       "      <th>funny_partner</th>\n",
       "      <th>ambition_partner</th>\n",
       "      <th>shared_interests_partner</th>\n",
       "      <th>interests_correlate</th>\n",
       "      <th>expected_happy_with_sd_people</th>\n",
       "      <th>expected_num_interested_in_me</th>\n",
       "      <th>like</th>\n",
       "      <th>guess_prob_liked</th>\n",
       "      <th>met</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>female</td>\n",
       "      <td>21.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>Asian/PacificIslander/Asian-American</td>\n",
       "      <td>European/Caucasian-American</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>female</td>\n",
       "      <td>21.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>Asian/PacificIslander/Asian-American</td>\n",
       "      <td>European/Caucasian-American</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>21.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>Asian/PacificIslander/Asian-American</td>\n",
       "      <td>Asian/PacificIslander/Asian-American</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>female</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Asian/PacificIslander/Asian-American</td>\n",
       "      <td>European/Caucasian-American</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.61</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>female</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>Asian/PacificIslander/Asian-American</td>\n",
       "      <td>Latino/HispanicAmerican</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   has_null  gender   age  age_o                                  race  \\\n",
       "0         0  female  21.0   27.0  Asian/PacificIslander/Asian-American   \n",
       "1         0  female  21.0   22.0  Asian/PacificIslander/Asian-American   \n",
       "2         1  female  21.0   22.0  Asian/PacificIslander/Asian-American   \n",
       "3         0  female  21.0   23.0  Asian/PacificIslander/Asian-American   \n",
       "4         0  female  21.0   24.0  Asian/PacificIslander/Asian-American   \n",
       "\n",
       "                                 race_o  importance_same_race  \\\n",
       "0           European/Caucasian-American                   2.0   \n",
       "1           European/Caucasian-American                   2.0   \n",
       "2  Asian/PacificIslander/Asian-American                   2.0   \n",
       "3           European/Caucasian-American                   2.0   \n",
       "4               Latino/HispanicAmerican                   2.0   \n",
       "\n",
       "   importance_same_religion  pref_o_attractive  pref_o_sincere  ...  \\\n",
       "0                       4.0               35.0            20.0  ...   \n",
       "1                       4.0               60.0             0.0  ...   \n",
       "2                       4.0               19.0            18.0  ...   \n",
       "3                       4.0               30.0             5.0  ...   \n",
       "4                       4.0               30.0            10.0  ...   \n",
       "\n",
       "   funny_partner  ambition_partner  shared_interests_partner  \\\n",
       "0            7.0               6.0                       5.0   \n",
       "1            8.0               5.0                       6.0   \n",
       "2            8.0               5.0                       7.0   \n",
       "3            7.0               6.0                       8.0   \n",
       "4            7.0               6.0                       6.0   \n",
       "\n",
       "   interests_correlate  expected_happy_with_sd_people  \\\n",
       "0                 0.14                            3.0   \n",
       "1                 0.54                            3.0   \n",
       "2                 0.16                            3.0   \n",
       "3                 0.61                            3.0   \n",
       "4                 0.21                            3.0   \n",
       "\n",
       "   expected_num_interested_in_me  like  guess_prob_liked  met  match  \n",
       "0                            2.0   7.0               6.0  0.0      0  \n",
       "1                            2.0   7.0               5.0  1.0      0  \n",
       "2                            2.0   7.0               NaN  1.0      1  \n",
       "3                            2.0   7.0               6.0  0.0      1  \n",
       "4                            2.0   6.0               6.0  0.0      1  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어떻게 생겼나 봅시다\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중간에 잘린 거 다 보고 싶어요\n",
    "pd.options.display.max_columns=40\n",
    "\n",
    "# 이거로 한 번에 표시될 컬럼 수를 조절할 수 있습니다\n",
    "# 당연히 rows도 가능해요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8378 entries, 0 to 8377\n",
      "Data columns (total 39 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   has_null                       8378 non-null   int64  \n",
      " 1   gender                         8378 non-null   object \n",
      " 2   age                            8283 non-null   float64\n",
      " 3   age_o                          8274 non-null   float64\n",
      " 4   race                           8315 non-null   object \n",
      " 5   race_o                         8305 non-null   object \n",
      " 6   importance_same_race           8299 non-null   float64\n",
      " 7   importance_same_religion       8299 non-null   float64\n",
      " 8   pref_o_attractive              8289 non-null   float64\n",
      " 9   pref_o_sincere                 8289 non-null   float64\n",
      " 10  pref_o_intelligence            8289 non-null   float64\n",
      " 11  pref_o_funny                   8280 non-null   float64\n",
      " 12  pref_o_ambitious               8271 non-null   float64\n",
      " 13  pref_o_shared_interests        8249 non-null   float64\n",
      " 14  attractive_o                   8166 non-null   float64\n",
      " 15  sincere_o                      8091 non-null   float64\n",
      " 16  intelligence_o                 8072 non-null   float64\n",
      " 17  funny_o                        8018 non-null   float64\n",
      " 18  ambitous_o                     7656 non-null   float64\n",
      " 19  shared_interests_o             7302 non-null   float64\n",
      " 20  attractive_important           8299 non-null   float64\n",
      " 21  sincere_important              8299 non-null   float64\n",
      " 22  intellicence_important         8299 non-null   float64\n",
      " 23  funny_important                8289 non-null   float64\n",
      " 24  ambtition_important            8279 non-null   float64\n",
      " 25  shared_interests_important     8257 non-null   float64\n",
      " 26  attractive_partner             8176 non-null   float64\n",
      " 27  sincere_partner                8101 non-null   float64\n",
      " 28  intelligence_partner           8082 non-null   float64\n",
      " 29  funny_partner                  8028 non-null   float64\n",
      " 30  ambition_partner               7666 non-null   float64\n",
      " 31  shared_interests_partner       7311 non-null   float64\n",
      " 32  interests_correlate            8220 non-null   float64\n",
      " 33  expected_happy_with_sd_people  8277 non-null   float64\n",
      " 34  expected_num_interested_in_me  1800 non-null   float64\n",
      " 35  like                           8138 non-null   float64\n",
      " 36  guess_prob_liked               8069 non-null   float64\n",
      " 37  met                            8003 non-null   float64\n",
      " 38  match                          8378 non-null   int64  \n",
      "dtypes: float64(34), int64(2), object(3)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# 그건 그렇고 저 안에 뭐가 들은 거죠 진짜\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- has_null : 변수 중에 null이 있는지 여부. 어떤 항목에 대한 무응답이 존재하는지의 여부로 보면 됨.\n",
    "- age / age_o : 본인 나이 / 상대 나이\n",
    "- race / race_o : 본인 인종 / 상대 인종\n",
    "- importance_same_race / importance_same_religion : 인종과 종교를 중시하는가\n",
    "- attractive(매력적), sincere(성실), intelligence(지적), funny(재미난), ambitious(야심찬), shared_interests(공통관심사)\n",
    "- -> pref_o_xxx : 상대방이 xxx 항목을 얼마나 중요시 하는가\n",
    "- -> xxx_o : 상대방이 본인에 대한 xxx 항목을 평가한 항목\n",
    "- -> xxx_important : 본인이 xxx 항목에 대해 얼마나 중요시 하는가\n",
    "- -> xxx_partner : 본인이 상대방에 대한 xxx 항목을 평가한 항목\n",
    "- interests_correlate : 관심사 연관도 (취미 등)\n",
    "- expected_happy_with_sd_people : 스피드 데이팅으로 만난 사람과 함께할 때 얼마나 좋은지\n",
    "- expected_num_interested_in_me : 얼마나 많은 사람들이 나한테 관심보일지에 대한 기대치\n",
    "- like : 파트너가 마음에 들었습니까?\n",
    "- guess_prob_liked : 파트너가 나를 좋아하지 않을까요?\n",
    "- met : 파트너를 스피드 데이팅 이벤트 이전에 만난 적이 있는지 여부 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>has_null</th>\n",
       "      <th>age</th>\n",
       "      <th>age_o</th>\n",
       "      <th>importance_same_race</th>\n",
       "      <th>importance_same_religion</th>\n",
       "      <th>pref_o_attractive</th>\n",
       "      <th>pref_o_sincere</th>\n",
       "      <th>pref_o_intelligence</th>\n",
       "      <th>pref_o_funny</th>\n",
       "      <th>pref_o_ambitious</th>\n",
       "      <th>pref_o_shared_interests</th>\n",
       "      <th>attractive_o</th>\n",
       "      <th>sincere_o</th>\n",
       "      <th>intelligence_o</th>\n",
       "      <th>funny_o</th>\n",
       "      <th>ambitous_o</th>\n",
       "      <th>shared_interests_o</th>\n",
       "      <th>attractive_important</th>\n",
       "      <th>sincere_important</th>\n",
       "      <th>intellicence_important</th>\n",
       "      <th>funny_important</th>\n",
       "      <th>ambtition_important</th>\n",
       "      <th>shared_interests_important</th>\n",
       "      <th>attractive_partner</th>\n",
       "      <th>sincere_partner</th>\n",
       "      <th>intelligence_partner</th>\n",
       "      <th>funny_partner</th>\n",
       "      <th>ambition_partner</th>\n",
       "      <th>shared_interests_partner</th>\n",
       "      <th>interests_correlate</th>\n",
       "      <th>expected_happy_with_sd_people</th>\n",
       "      <th>expected_num_interested_in_me</th>\n",
       "      <th>like</th>\n",
       "      <th>guess_prob_liked</th>\n",
       "      <th>met</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8378.00</td>\n",
       "      <td>8283.00</td>\n",
       "      <td>8274.00</td>\n",
       "      <td>8299.00</td>\n",
       "      <td>8299.00</td>\n",
       "      <td>8289.00</td>\n",
       "      <td>8289.00</td>\n",
       "      <td>8289.00</td>\n",
       "      <td>8280.00</td>\n",
       "      <td>8271.00</td>\n",
       "      <td>8249.00</td>\n",
       "      <td>8166.00</td>\n",
       "      <td>8091.00</td>\n",
       "      <td>8072.00</td>\n",
       "      <td>8018.00</td>\n",
       "      <td>7656.00</td>\n",
       "      <td>7302.00</td>\n",
       "      <td>8299.00</td>\n",
       "      <td>8299.00</td>\n",
       "      <td>8299.00</td>\n",
       "      <td>8289.00</td>\n",
       "      <td>8279.00</td>\n",
       "      <td>8257.00</td>\n",
       "      <td>8176.00</td>\n",
       "      <td>8101.00</td>\n",
       "      <td>8082.00</td>\n",
       "      <td>8028.00</td>\n",
       "      <td>7666.00</td>\n",
       "      <td>7311.00</td>\n",
       "      <td>8220.00</td>\n",
       "      <td>8277.00</td>\n",
       "      <td>1800.00</td>\n",
       "      <td>8138.00</td>\n",
       "      <td>8069.00</td>\n",
       "      <td>8003.00</td>\n",
       "      <td>8378.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.87</td>\n",
       "      <td>26.36</td>\n",
       "      <td>26.36</td>\n",
       "      <td>3.78</td>\n",
       "      <td>3.65</td>\n",
       "      <td>22.50</td>\n",
       "      <td>17.40</td>\n",
       "      <td>20.27</td>\n",
       "      <td>17.46</td>\n",
       "      <td>10.69</td>\n",
       "      <td>11.85</td>\n",
       "      <td>6.19</td>\n",
       "      <td>7.18</td>\n",
       "      <td>7.37</td>\n",
       "      <td>6.40</td>\n",
       "      <td>6.78</td>\n",
       "      <td>5.47</td>\n",
       "      <td>22.51</td>\n",
       "      <td>17.40</td>\n",
       "      <td>20.27</td>\n",
       "      <td>17.46</td>\n",
       "      <td>10.68</td>\n",
       "      <td>11.85</td>\n",
       "      <td>6.19</td>\n",
       "      <td>7.18</td>\n",
       "      <td>7.37</td>\n",
       "      <td>6.40</td>\n",
       "      <td>6.78</td>\n",
       "      <td>5.47</td>\n",
       "      <td>0.20</td>\n",
       "      <td>5.53</td>\n",
       "      <td>5.57</td>\n",
       "      <td>6.13</td>\n",
       "      <td>5.21</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.33</td>\n",
       "      <td>3.57</td>\n",
       "      <td>3.56</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.81</td>\n",
       "      <td>12.57</td>\n",
       "      <td>7.04</td>\n",
       "      <td>6.78</td>\n",
       "      <td>6.09</td>\n",
       "      <td>6.13</td>\n",
       "      <td>6.36</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.79</td>\n",
       "      <td>2.16</td>\n",
       "      <td>12.59</td>\n",
       "      <td>7.05</td>\n",
       "      <td>6.78</td>\n",
       "      <td>6.09</td>\n",
       "      <td>6.12</td>\n",
       "      <td>6.36</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.79</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.73</td>\n",
       "      <td>4.76</td>\n",
       "      <td>1.84</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>17.39</td>\n",
       "      <td>15.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>9.52</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>17.39</td>\n",
       "      <td>15.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>9.52</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>18.37</td>\n",
       "      <td>20.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.64</td>\n",
       "      <td>6.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>18.18</td>\n",
       "      <td>20.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.64</td>\n",
       "      <td>6.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.21</td>\n",
       "      <td>6.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>23.81</td>\n",
       "      <td>20.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>23.81</td>\n",
       "      <td>20.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.43</td>\n",
       "      <td>7.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>53.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>10.50</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>53.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.91</td>\n",
       "      <td>10.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       has_null      age    age_o  importance_same_race  \\\n",
       "count   8378.00  8283.00  8274.00               8299.00   \n",
       "mean       0.87    26.36    26.36                  3.78   \n",
       "std        0.33     3.57     3.56                  2.85   \n",
       "min        0.00    18.00    18.00                  0.00   \n",
       "25%        1.00    24.00    24.00                  1.00   \n",
       "50%        1.00    26.00    26.00                  3.00   \n",
       "75%        1.00    28.00    28.00                  6.00   \n",
       "max        1.00    55.00    55.00                 10.00   \n",
       "\n",
       "       importance_same_religion  pref_o_attractive  pref_o_sincere  \\\n",
       "count                   8299.00            8289.00         8289.00   \n",
       "mean                       3.65              22.50           17.40   \n",
       "std                        2.81              12.57            7.04   \n",
       "min                        1.00               0.00            0.00   \n",
       "25%                        1.00              15.00           15.00   \n",
       "50%                        3.00              20.00           18.37   \n",
       "75%                        6.00              25.00           20.00   \n",
       "max                       10.00             100.00           60.00   \n",
       "\n",
       "       pref_o_intelligence  pref_o_funny  pref_o_ambitious  \\\n",
       "count              8289.00       8280.00           8271.00   \n",
       "mean                 20.27         17.46             10.69   \n",
       "std                   6.78          6.09              6.13   \n",
       "min                   0.00          0.00              0.00   \n",
       "25%                  17.39         15.00              5.00   \n",
       "50%                  20.00         18.00             10.00   \n",
       "75%                  23.81         20.00             15.00   \n",
       "max                  50.00         50.00             53.00   \n",
       "\n",
       "       pref_o_shared_interests  attractive_o  sincere_o  intelligence_o  \\\n",
       "count                  8249.00       8166.00    8091.00         8072.00   \n",
       "mean                     11.85          6.19       7.18            7.37   \n",
       "std                       6.36          1.95       1.74            1.55   \n",
       "min                       0.00          0.00       0.00            0.00   \n",
       "25%                       9.52          5.00       6.00            6.00   \n",
       "50%                      10.64          6.00       7.00            7.00   \n",
       "75%                      16.00          8.00       8.00            8.00   \n",
       "max                      30.00         10.50      10.00           10.00   \n",
       "\n",
       "       funny_o  ambitous_o  shared_interests_o  attractive_important  \\\n",
       "count  8018.00     7656.00             7302.00               8299.00   \n",
       "mean      6.40        6.78                5.47                 22.51   \n",
       "std       1.95        1.79                2.16                 12.59   \n",
       "min       0.00        0.00                0.00                  0.00   \n",
       "25%       5.00        6.00                4.00                 15.00   \n",
       "50%       7.00        7.00                6.00                 20.00   \n",
       "75%       8.00        8.00                7.00                 25.00   \n",
       "max      11.00       10.00               10.00                100.00   \n",
       "\n",
       "       sincere_important  intellicence_important  funny_important  \\\n",
       "count            8299.00                 8299.00          8289.00   \n",
       "mean               17.40                   20.27            17.46   \n",
       "std                 7.05                    6.78             6.09   \n",
       "min                 0.00                    0.00             0.00   \n",
       "25%                15.00                   17.39            15.00   \n",
       "50%                18.18                   20.00            18.00   \n",
       "75%                20.00                   23.81            20.00   \n",
       "max                60.00                   50.00            50.00   \n",
       "\n",
       "       ambtition_important  shared_interests_important  attractive_partner  \\\n",
       "count              8279.00                     8257.00             8176.00   \n",
       "mean                 10.68                       11.85                6.19   \n",
       "std                   6.12                        6.36                1.95   \n",
       "min                   0.00                        0.00                0.00   \n",
       "25%                   5.00                        9.52                5.00   \n",
       "50%                  10.00                       10.64                6.00   \n",
       "75%                  15.00                       16.00                8.00   \n",
       "max                  53.00                       30.00               10.00   \n",
       "\n",
       "       sincere_partner  intelligence_partner  funny_partner  ambition_partner  \\\n",
       "count          8101.00               8082.00        8028.00           7666.00   \n",
       "mean              7.18                  7.37           6.40              6.78   \n",
       "std               1.74                  1.55           1.95              1.79   \n",
       "min               0.00                  0.00           0.00              0.00   \n",
       "25%               6.00                  6.00           5.00              6.00   \n",
       "50%               7.00                  7.00           7.00              7.00   \n",
       "75%               8.00                  8.00           8.00              8.00   \n",
       "max              10.00                 10.00          10.00             10.00   \n",
       "\n",
       "       shared_interests_partner  interests_correlate  \\\n",
       "count                   7311.00              8220.00   \n",
       "mean                       5.47                 0.20   \n",
       "std                        2.16                 0.30   \n",
       "min                        0.00                -0.83   \n",
       "25%                        4.00                -0.02   \n",
       "50%                        6.00                 0.21   \n",
       "75%                        7.00                 0.43   \n",
       "max                       10.00                 0.91   \n",
       "\n",
       "       expected_happy_with_sd_people  expected_num_interested_in_me     like  \\\n",
       "count                        8277.00                        1800.00  8138.00   \n",
       "mean                            5.53                           5.57     6.13   \n",
       "std                             1.73                           4.76     1.84   \n",
       "min                             1.00                           0.00     0.00   \n",
       "25%                             5.00                           2.00     5.00   \n",
       "50%                             6.00                           4.00     6.00   \n",
       "75%                             7.00                           8.00     7.00   \n",
       "max                            10.00                          20.00    10.00   \n",
       "\n",
       "       guess_prob_liked      met    match  \n",
       "count           8069.00  8003.00  8378.00  \n",
       "mean               5.21     0.05     0.16  \n",
       "std                2.13     0.28     0.37  \n",
       "min                0.00     0.00     0.00  \n",
       "25%                4.00     0.00     0.00  \n",
       "50%                5.00     0.00     0.00  \n",
       "75%                7.00     0.00     0.00  \n",
       "max               10.00     8.00     1.00  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 통계적 정보 확인\n",
    "round(data.describe(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 자세히 보면 파트너를 평가하는 항목들은 max가 10인데\n",
    "- 성격같은 중요도는 max가 100이나 60, 50 등 제각각인 상태임\n",
    "- 이런 경우 데이터가 무슨 기준으로 수집됐는지를 알아야 도움이 됩니다.\n",
    "\n",
    "\n",
    "- 교재 설명을 더 가져오면\n",
    "- 평가 관련 변수는 0~10점을 각각의 항목에 넣습니다\n",
    "- 근데 pref_o_funny, pref_o_ambitious, pref_o_shared_interests, attractive_o, sincere_o, intelligence_o 등 중요도 변수는 합쳐서 100점인 상태\n",
    "- 그러니까 상대방이 각 항목에 대한 중요도 변수들의 합은 100이 되어야 한다는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "has_null                         0.000000\n",
       "gender                           0.000000\n",
       "age                              0.011339\n",
       "age_o                            0.012413\n",
       "race                             0.007520\n",
       "race_o                           0.008713\n",
       "importance_same_race             0.009429\n",
       "importance_same_religion         0.009429\n",
       "pref_o_attractive                0.010623\n",
       "pref_o_sincere                   0.010623\n",
       "pref_o_intelligence              0.010623\n",
       "pref_o_funny                     0.011697\n",
       "pref_o_ambitious                 0.012772\n",
       "pref_o_shared_interests          0.015397\n",
       "attractive_o                     0.025304\n",
       "sincere_o                        0.034256\n",
       "intelligence_o                   0.036524\n",
       "funny_o                          0.042970\n",
       "ambitous_o                       0.086178\n",
       "shared_interests_o               0.128432\n",
       "attractive_important             0.009429\n",
       "sincere_important                0.009429\n",
       "intellicence_important           0.009429\n",
       "funny_important                  0.010623\n",
       "ambtition_important              0.011817\n",
       "shared_interests_important       0.014443\n",
       "attractive_partner               0.024111\n",
       "sincere_partner                  0.033063\n",
       "intelligence_partner             0.035331\n",
       "funny_partner                    0.041776\n",
       "ambition_partner                 0.084984\n",
       "shared_interests_partner         0.127357\n",
       "interests_correlate              0.018859\n",
       "expected_happy_with_sd_people    0.012055\n",
       "expected_num_interested_in_me    0.785152\n",
       "like                             0.028646\n",
       "guess_prob_liked                 0.036882\n",
       "met                              0.044760\n",
       "match                            0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 일단 결측치나 봅시다\n",
    "data.isna().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그냥 결측치가 여기저기 넘쳐나는 상태인데 다행인 건 대체로 5% 미만임\n",
    "- 그건 그렇고 XGBoost는 어쨌든 트리 베이스라 결측치를 채워야하는 게 까다로워요\n",
    "- 윈저라이징을 하긴 할 건데 중요도와 관련된 변수들은 결측치를 없앨겁니다\n",
    "- 나중에 중요도 x 점수로 계산할 일이 있어서 버릴겁니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['pref_o_attractive', 'pref_o_sincere', 'pref_o_intelligence', 'pref_o_funny', 'pref_o_ambitious', 'pref_o_shared_interests', 'attractive_important',\n",
    "'sincere_important', 'intellicence_important', 'funny_important', 'ambtition_important', 'shared_interests_important'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나머지는 -99로 채울겁니다\n",
    "# '알 수 없음'이라는 의미로 받아주세요\n",
    "\n",
    "data = data.fillna(-99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우선 나이 차부터 시작해볼까요\n",
    "# 근데 결측치를 -99로 채운 거 있어서 막 계산하면 안 됩니다\n",
    "\n",
    "def age_gap(x):\n",
    "    if x['age'] == -99:\n",
    "        return -99\n",
    "    elif x['age_o'] == -99:\n",
    "        return -99\n",
    "    elif x['gender'] == 'feamle':\n",
    "        return x['age_o'] - x['age']\n",
    "    else:\n",
    "        return x['age'] - x['age_o']\n",
    "\n",
    "# -99가 있으면 -99를 리턴하고\n",
    "# 남자가 연상일 때 양수, 여자가 연상일 때 음수가 리턴되는 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나이차를 적용시켜봅시다\n",
    "data['age_gap'] = data.apply(age_gap, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 근데 나이가 차이나는 그 자체를 중요하게 생각할 수도 있잖아요?\n",
    "# 그래서 절댓값으로 나이차만 저장하는 컬럼 하나 더 만들겁니다\n",
    "\n",
    "data['age_gap_abs'] = abs(data['age_gap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인종 관련 함수\n",
    "# 마찬가지로 결측치 있으면 -99, 둘이 맞으면 1, 아니면 -1\n",
    "\n",
    "def same_race(x):\n",
    "    if x['race'] == -99:\n",
    "        return -99\n",
    "    elif x['race_o'] == -99:\n",
    "        return -99\n",
    "    elif x['race'] == x['race_o']:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적용해볼까요\n",
    "data['same_race'] = data.apply(same_race, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 근데 importance_same_race 라는 컬럼도 있음\n",
    "# 인종이 같으면 양수, 아니면 음수를 리턴할건데 숫자가 클 수록 당연히 중요도가 높을 것임\n",
    "def same_race_point(x):\n",
    "    if x['same_race'] == -99:\n",
    "        return -99\n",
    "    else:\n",
    "        return x['same_race'] * x['importance_same_race']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이거도 적용해봅시다\n",
    "data['same_race_point'] = data.apply(same_race_point, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 근데 왜 인종이 다르면 음수를 리턴할까\n",
    "- 인종 신경 안 쓰는 사람은 'same_race'에 0을 갖고 있을텐데\n",
    "- 만약 인종이 다르다는 걸 0으로 리턴해버리면 인종 신경 안 쓰는 사람이랑 구별하기 어려울 것입니다.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 평가하는 함수\n",
    "\n",
    "def rating(data, importance, score):\n",
    "    if data[importance] == -99:\n",
    "        return -99\n",
    "    elif data[score] == -99:\n",
    "        return -99\n",
    "    else:\n",
    "        return data[importance] * data[score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pref_o_attractive', 'pref_o_sincere', 'pref_o_intelligence',\n",
       "       'pref_o_funny', 'pref_o_ambitious', 'pref_o_shared_interests'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 음... importance에 들어갈 컬럼들을 따로 만들면 좋을 거 같은데\n",
    "\n",
    "data.columns[8:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저런 식으로 몇몇 컬럼들을 묶어봅시다\n",
    "\n",
    "partner_imp = data.columns[8:14] # 상대방의 중요도\n",
    "partner_rate_me = data.columns[14:20] # 상대방이 본인을 평가\n",
    "my_imp = data.columns[20:26] # 나의 중요도\n",
    "my_rate_partner = data.columns[26:32] # 상대방에 대한 본인의 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상대방 관련 새 변수 이름을 저장하는 리스트\n",
    "new_label_partner = ['attractive_p', 'sincere_partner_p', 'intelligence_p', 'funny_p', 'ambition_p', 'shared_interests_p']\n",
    "\n",
    "# 나와 관련된 새 변수 이름을 저장하는 리스트\n",
    "new_label_me = ['attractive_m', 'sincere_partner_m', 'intelligence_m', 'funny_m', 'ambition_m', 'shared_interests_m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 rating 함수를 이용해 중요도 점수를 계산해봅시다\n",
    "\n",
    "for i, j, k in zip(new_label_partner, partner_imp, partner_rate_me):\n",
    "    data[i] = data.apply(lambda x: rating(x, j, k), axis=1)\n",
    "\n",
    "\n",
    "for i, j, k in zip(new_label_me, my_imp, my_rate_partner):\n",
    "    data[i] = data.apply(lambda x: rating(x, j, k), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마지막으로 더미를 만들어주면 끝 (One-Hot Encoding)\n",
    "\n",
    "data = pd.get_dummies(data, columns=['gender', 'race', 'race_o'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>has_null</th>\n",
       "      <th>age</th>\n",
       "      <th>age_o</th>\n",
       "      <th>importance_same_race</th>\n",
       "      <th>importance_same_religion</th>\n",
       "      <th>pref_o_attractive</th>\n",
       "      <th>pref_o_sincere</th>\n",
       "      <th>pref_o_intelligence</th>\n",
       "      <th>pref_o_funny</th>\n",
       "      <th>pref_o_ambitious</th>\n",
       "      <th>pref_o_shared_interests</th>\n",
       "      <th>attractive_o</th>\n",
       "      <th>sincere_o</th>\n",
       "      <th>intelligence_o</th>\n",
       "      <th>funny_o</th>\n",
       "      <th>ambitous_o</th>\n",
       "      <th>shared_interests_o</th>\n",
       "      <th>attractive_important</th>\n",
       "      <th>sincere_important</th>\n",
       "      <th>intellicence_important</th>\n",
       "      <th>...</th>\n",
       "      <th>sincere_partner_p</th>\n",
       "      <th>intelligence_p</th>\n",
       "      <th>funny_p</th>\n",
       "      <th>ambition_p</th>\n",
       "      <th>shared_interests_p</th>\n",
       "      <th>attractive_m</th>\n",
       "      <th>sincere_partner_m</th>\n",
       "      <th>intelligence_m</th>\n",
       "      <th>funny_m</th>\n",
       "      <th>ambition_m</th>\n",
       "      <th>shared_interests_m</th>\n",
       "      <th>gender_male</th>\n",
       "      <th>race_Black/AfricanAmerican</th>\n",
       "      <th>race_European/Caucasian-American</th>\n",
       "      <th>race_Latino/HispanicAmerican</th>\n",
       "      <th>race_Other</th>\n",
       "      <th>race_o_Black/AfricanAmerican</th>\n",
       "      <th>race_o_European/Caucasian-American</th>\n",
       "      <th>race_o_Latino/HispanicAmerican</th>\n",
       "      <th>race_o_Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>160.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>180.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   has_null   age  age_o  importance_same_race  importance_same_religion  \\\n",
       "0         0  21.0   27.0                   2.0                       4.0   \n",
       "1         0  21.0   22.0                   2.0                       4.0   \n",
       "2         1  21.0   22.0                   2.0                       4.0   \n",
       "3         0  21.0   23.0                   2.0                       4.0   \n",
       "4         0  21.0   24.0                   2.0                       4.0   \n",
       "\n",
       "   pref_o_attractive  pref_o_sincere  pref_o_intelligence  pref_o_funny  \\\n",
       "0               35.0            20.0                 20.0          20.0   \n",
       "1               60.0             0.0                  0.0          40.0   \n",
       "2               19.0            18.0                 19.0          18.0   \n",
       "3               30.0             5.0                 15.0          40.0   \n",
       "4               30.0            10.0                 20.0          10.0   \n",
       "\n",
       "   pref_o_ambitious  pref_o_shared_interests  attractive_o  sincere_o  \\\n",
       "0               0.0                      5.0           6.0        8.0   \n",
       "1               0.0                      0.0           7.0        8.0   \n",
       "2              14.0                     12.0          10.0       10.0   \n",
       "3               5.0                      5.0           7.0        8.0   \n",
       "4              10.0                     20.0           8.0        7.0   \n",
       "\n",
       "   intelligence_o  funny_o  ambitous_o  shared_interests_o  \\\n",
       "0             8.0      8.0         8.0                 6.0   \n",
       "1            10.0      7.0         7.0                 5.0   \n",
       "2            10.0     10.0        10.0                10.0   \n",
       "3             9.0      8.0         9.0                 8.0   \n",
       "4             9.0      6.0         9.0                 7.0   \n",
       "\n",
       "   attractive_important  sincere_important  intellicence_important  ...  \\\n",
       "0                  15.0               20.0                    20.0  ...   \n",
       "1                  15.0               20.0                    20.0  ...   \n",
       "2                  15.0               20.0                    20.0  ...   \n",
       "3                  15.0               20.0                    20.0  ...   \n",
       "4                  15.0               20.0                    20.0  ...   \n",
       "\n",
       "   sincere_partner_p  intelligence_p  funny_p  ambition_p  shared_interests_p  \\\n",
       "0              160.0           160.0    160.0         0.0                30.0   \n",
       "1                0.0             0.0    280.0         0.0                 0.0   \n",
       "2              180.0           190.0    180.0       140.0               120.0   \n",
       "3               40.0           135.0    320.0        45.0                40.0   \n",
       "4               70.0           180.0     60.0        90.0               140.0   \n",
       "\n",
       "   attractive_m  sincere_partner_m  intelligence_m  funny_m  ambition_m  \\\n",
       "0          90.0              180.0           140.0    105.0        90.0   \n",
       "1         105.0              160.0           140.0    120.0        75.0   \n",
       "2          75.0              160.0           180.0    120.0        75.0   \n",
       "3         105.0              120.0           160.0    105.0        90.0   \n",
       "4          75.0              120.0           140.0    105.0        90.0   \n",
       "\n",
       "   shared_interests_m  gender_male  race_Black/AfricanAmerican  \\\n",
       "0                75.0            0                           0   \n",
       "1                90.0            0                           0   \n",
       "2               105.0            0                           0   \n",
       "3               120.0            0                           0   \n",
       "4                90.0            0                           0   \n",
       "\n",
       "   race_European/Caucasian-American  race_Latino/HispanicAmerican  race_Other  \\\n",
       "0                                 0                             0           0   \n",
       "1                                 0                             0           0   \n",
       "2                                 0                             0           0   \n",
       "3                                 0                             0           0   \n",
       "4                                 0                             0           0   \n",
       "\n",
       "   race_o_Black/AfricanAmerican  race_o_European/Caucasian-American  \\\n",
       "0                             0                                   1   \n",
       "1                             0                                   1   \n",
       "2                             0                                   0   \n",
       "3                             0                                   1   \n",
       "4                             0                                   0   \n",
       "\n",
       "   race_o_Latino/HispanicAmerican  race_o_Other  \n",
       "0                               0             0  \n",
       "1                               0             0  \n",
       "2                               0             0  \n",
       "3                               0             0  \n",
       "4                               1             0  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 트레이닝 시작함\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('match', axis=1), data['match'], test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "# 하이퍼파라미터를 3개만 지정했는데 공식문서 보면 알겠지만..... 겁나 많음\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBClassifier(n_estimators=500, max_depth=5, random_state=100)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8628536285362853"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    8130.000000\n",
       "mean        0.164822\n",
       "std         0.371042\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         0.000000\n",
       "max         1.000000\n",
       "Name: match, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 높아 보이는데 과연?\n",
    "data['match'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- match 평균이 16% 수준인데 86% 정확도가 나왔다고?\n",
    "- 이러면 나머지 84%는 매칭도 안 됐다는 얘기인데 뭐냐 이건"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1293   72]\n",
      " [ 151  110]]\n"
     ]
    }
   ],
   "source": [
    "# 혼동행렬 까봅니다\n",
    "\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1종 오류가 72건, 2종 오류가 151건"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92      1365\n",
      "           1       0.60      0.42      0.50       261\n",
      "\n",
      "    accuracy                           0.86      1626\n",
      "   macro avg       0.75      0.68      0.71      1626\n",
      "weighted avg       0.85      0.86      0.85      1626\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification_report를 사용하면 혼동 행렬 정보 파악이 쉬워짐\n",
    "\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 웬만해서는 예측하고자 하는 값을 '1'로 설정할테니 '1'을 중점적으로 봅시다\n",
    "- 아무리 봐도 점수들이... 많이 낮은데?\n",
    "- 정밀도(precision) : 1로 예측한 것 중 얼마만큼 실제로 1인가 (TP/TP+FP) --- 1종 오류랑 관련 있음\n",
    "- 재현율(recall) : 실제 1 중 얼마나 1로 예측했는가 (TP/TP+FN) --- 2종 오류랑 관련 있음\n",
    "- F1-Score : 정밀도와 재현율의 조화평균(주어진 수들의 역수의 산술평균의 역수) -> 2 * precision * recall / precision + recall\n",
    "- 만약 1종 오류가 중요하다면 정밀도를 2종 오류가 중요하다면 재현율을 신경써야함\n",
    "- 특별히 뭐가 더 중요하지 않다라고 생각된다면 F1-Score로 판단하는게 무난함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 경사하강법\n",
    "- 머신러닝이 학습할 때 최소의 오차를 찾도록 만드는 방법\n",
    "- 오차 함수에 대한 경사도(미분계수)를 기준으로 매개변수를 반복적으로 이동해 최소 오차를 찾음\n",
    "- 문제가 미분계수를 0으로 할만한 곳이 2개 이상이 되면.. 그러니까 지역 최솟값이랑 전역 최솟값이 같이 튀어나올 수도 있는데\n",
    "- 만약 학습률이 충분하지 않다면 지역 최솟값을 전역 최솟값으로 오해해서 원하는 값이 안 나올 수 있어요\n",
    "- 근데 이거 어떻게 찾죠 사람이 일일히 찾나?\n",
    "- 그러라고 gridSearch가 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    'learning_rate' : [0.01, 0.1, 0.3], # 매개변수를 얼만큼 이동하면서 찾을까요\n",
    "    'max_depth' : [5, 7, 10], # 트리의 깊이\n",
    "    'subsample' : [0.5, 0.7, 1], # 모델을 학습할 때 일부 데이터만 써서 트리를 만듬. 0.5인 경우 데이터의 절반씩 랜덤 추출함. 오버피팅 방지도 좋음.\n",
    "    'n_estimators' : [300, 500, 1000] # 전체 나무의 개수\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_bin=None,\n",
       "                                     max_ca...\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=None, reg_alpha=None,\n",
       "                                     reg_lambda=None, ...),\n",
       "             n_jobs=1,\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.01, 0.1, 0.3],\n",
       "                         &#x27;max_depth&#x27;: [5, 7, 10],\n",
       "                         &#x27;n_estimators&#x27;: [300, 500, 1000],\n",
       "                         &#x27;subsample&#x27;: [0.5, 0.7, 1]},\n",
       "             scoring=&#x27;f1&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_bin=None,\n",
       "                                     max_ca...\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=None, reg_alpha=None,\n",
       "                                     reg_lambda=None, ...),\n",
       "             n_jobs=1,\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.01, 0.1, 0.3],\n",
       "                         &#x27;max_depth&#x27;: [5, 7, 10],\n",
       "                         &#x27;n_estimators&#x27;: [300, 500, 1000],\n",
       "                         &#x27;subsample&#x27;: [0.5, 0.7, 1]},\n",
       "             scoring=&#x27;f1&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, gamma=None,\n",
       "              gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None,\n",
       "              reg_alpha=None, reg_lambda=None, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, gamma=None,\n",
       "              gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None,\n",
       "              reg_alpha=None, reg_lambda=None, ...)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_bin=None,\n",
       "                                     max_ca...\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=None, reg_alpha=None,\n",
       "                                     reg_lambda=None, ...),\n",
       "             n_jobs=1,\n",
       "             param_grid={'learning_rate': [0.01, 0.1, 0.3],\n",
       "                         'max_depth': [5, 7, 10],\n",
       "                         'n_estimators': [300, 500, 1000],\n",
       "                         'subsample': [0.5, 0.7, 1]},\n",
       "             scoring='f1')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이번에는 GridSearch를 이용해 갑니다\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "gs_model = GridSearchCV(model, parameters, n_jobs=1, scoring='f1', cv=5)\n",
    "\n",
    "# n_jobs : 사용할 코어 수\n",
    "# scoring : 어떤 기준으로 할까요\n",
    "# cv : k-fold value\n",
    "\n",
    "\n",
    "gs_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.3, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 0.7}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 과연 우리의 최적 하이퍼 파라미터는 뭘까요\n",
    "\n",
    "gs_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs_model은 GridSearchCV가 완료된 상태라 최적 하이퍼 파라미터가 들어가있음\n",
    "pred = gs_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8646986469864698"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92      1365\n",
      "           1       0.61      0.44      0.51       261\n",
      "\n",
      "    accuracy                           0.86      1626\n",
      "   macro avg       0.75      0.69      0.72      1626\n",
      "weighted avg       0.85      0.86      0.86      1626\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이런다고 살림살이 나아지진 않지만 그래도 뭐.... 개선이 되긴 됐네"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=5, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=1000,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=100,\n",
       "              reg_alpha=0, reg_lambda=1, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=5, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=1000,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=100,\n",
       "              reg_alpha=0, reg_lambda=1, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.3, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=5, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=1000,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=100,\n",
       "              reg_alpha=0, reg_lambda=1, ...)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 근데 아깐 자동이었으니까 이번엔 수동으로 최적 하이퍼 파라미터를 넣어봅시다\n",
    "\n",
    "\n",
    "model = xgb.XGBClassifier(learning_rate = 0.3, max_depth=5, n_estimators=1000, subsample=0.7, random_state=100)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01150109, 0.01082742, 0.01022119, 0.00718562, 0.00875766,\n",
       "       0.0123986 , 0.00954044, 0.01321438, 0.0093683 , 0.01306725,\n",
       "       0.01172539, 0.04970133, 0.01383632, 0.01136932, 0.03743635,\n",
       "       0.01084364, 0.02035189, 0.01262651, 0.01013969, 0.01491023,\n",
       "       0.00823365, 0.01112775, 0.01111076, 0.02414486, 0.01220307,\n",
       "       0.00887889, 0.02382516, 0.01218321, 0.01428757, 0.01086993,\n",
       "       0.01052965, 0.01879149, 0.07235699, 0.02200928, 0.0256325 ,\n",
       "       0.00951941, 0.01001255, 0.00818915, 0.01228332, 0.0102939 ,\n",
       "       0.01075684, 0.01275628, 0.01316534, 0.00996484, 0.01135542,\n",
       "       0.01015693, 0.00892564, 0.00885974, 0.00880156, 0.00861083,\n",
       "       0.00977737, 0.01543015, 0.03114583, 0.01392106, 0.02247188,\n",
       "       0.03680198, 0.04524185, 0.0137238 , 0.04118284, 0.03144409],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 피처의 중요도를 보는데 이러면 뭐가 뭔지 모르잖아\n",
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>has_null</td>\n",
       "      <td>0.011501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>age</td>\n",
       "      <td>0.010827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age_o</td>\n",
       "      <td>0.010221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>importance_same_race</td>\n",
       "      <td>0.007186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>importance_same_religion</td>\n",
       "      <td>0.008758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   features    values\n",
       "0                  has_null  0.011501\n",
       "1                       age  0.010827\n",
       "2                     age_o  0.010221\n",
       "3      importance_same_race  0.007186\n",
       "4  importance_same_religion  0.008758"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터프레임으로 만들어서 보면 되겠다\n",
    "\n",
    "feature_imp = pd.DataFrame({'features':X_train.columns, 'values':model.feature_importances_})\n",
    "feature_imp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='values', ylabel='features'>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 내림차순으로 그래프를 뽑아봅시다\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.barplot(x='values', y='features', data=feature_imp.sort_values(by='values', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 보충\n",
    "\n",
    "#### 트리 모델은 어떻게 진화했는가\n",
    "결정트리 > 배깅 > 랜덤포레스트 > 부스팅 > 경사 부스팅 > XG 부스팅\n",
    "\n",
    "\n",
    "##### 배깅\n",
    "- 부트스트랩 훈련셋을 사용하는 트리모델\n",
    "- 부트스트랩 : 데이터 일부분을 무작위로 반복 추출하는 방법\n",
    "- 이렇게 여러번 뽑아 여러 부분 집합을 쓰면 오버피팅을 방지함\n",
    "\n",
    "##### 랜덤포레스트\n",
    "- 배깅을 발전시킨 모델\n",
    "- 데이터와 변수 일부를 사용해 여러 트리를 만듦\n",
    "\n",
    "##### 부스팅\n",
    "- 랜덤 포레스트에서 발전함\n",
    "- 랜덤 포레스트는 트리가 독립적인데 부스팅은 그렇지 않다는 게 차이점\n",
    "- 부스팅의 경우에는 트리를 만들 때 이전 트리의 정보를 이용해 만듦\n",
    "\n",
    "##### 에이다부스트(AdaBoost)\n",
    "- 단계적으로 트리를 만들 때 이전 단계의 분류 결과에 따라 가중치를 부여하거나 수정함\n",
    "- 이전 트리에서 잘못 분류된 데이터한테 높은 가중치, 후속 트리에서 가중치가 높은 데이터를 분류하는데 우선 순위를 둔다\n",
    "\n",
    "##### 경사부스팅\n",
    "- 부스팅에 경사하강법을 접목시킨 방법\n",
    "- 이전 모델의 에러를 기반으로 다음 트리를 만들어감\n",
    "- 여기 해당하는 게 XGBoost, LightGBM, CatBoost 등\n",
    "\n",
    "##### XGBoost\n",
    "- 경사부스팅에 계산 성능 최적화와 알고리즘 개선을 꾀함\n",
    "- 병렬화, 분산 컴퓨팅, 캐시 최적화 등을 활용하여 계산 속도를 개선했음\n",
    "- 2차 도함수(2번 미분)를 활용해 더 적절한 이동방향과 이동크기를 찾아내 빠른 시간에 전역 최솟값에 도달하도록 함\n",
    "- 정규화 파라미터 모델 지원하는 것도 장점. LASSO(L1), Ridge(L2) 정규화 하이퍼 파라미터를 지원함. (오버피팅 방지)\n",
    "\n",
    "\n",
    "##### 나아가기\n",
    "- 그리드 서치 수행할 때 다른 하이퍼파라미터 값을 넣어 더 나은 예측을 보일 수 있는 조합이 있는지 찾아봅시다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "189f630ccbee5b0c2b394a5837f4edcfb3fd6675fa752a6b628b89f66389bcc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
